{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.Loading the IMDB Dataset\n",
    "* 2.Preparing the data\n",
    "* 3.Building our Network\n",
    "    * 3.1 The model definition\n",
    "    * 3.2 Compiling the model\n",
    "    * 3.3 Validating our approach\n",
    "    * 3.4 Training the model\n",
    "    * 3.5 Plotting the training and the validation loss\n",
    "    * 3.6 Plotting the training and the validation accuracy\n",
    "    * 3.7 Retraining a model from scratch\n",
    "* 4.Further Experiments\n",
    "* 5.Wrapping up\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This little project is to use deep learning to classify  Movie reviews in the Internet Movie DataBase (IMDB) as negative or positive. \n",
    " \n",
    "* The IMDB dataset: a set of 50,000 highly polarized reviews from the Internet Movie Database. They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Loading the IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindegila/.conda/envs/mytensor/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels),(test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument **num_words=10000** means we’ll only keep the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows us to work with vector data of manageable size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables **train_data and test_data** are lists of reviews; each review is a list of\n",
    "word indices (encoding a sequence of words). **train_labels and test_labels** are\n",
    "lists of 0s and 1s, where 0 stands for negative and 1 stands for positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 281)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[15000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0], train_labels[15000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we’re restricting yourself to the top 10,000 most frequent words, no word\n",
    "index will exceed 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's decode one of the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fawn': 34701,\n",
       " 'tsukino': 52006,\n",
       " 'nunnery': 52007,\n",
       " 'sonja': 16816,\n",
       " 'vani': 63951,\n",
       " 'woods': 1408,\n",
       " 'spiders': 16115,\n",
       " 'hanging': 2345,\n",
       " 'woody': 2289,\n",
       " 'trawling': 52008,\n",
       " \"hold's\": 52009,\n",
       " 'comically': 11307,\n",
       " 'localized': 40830,\n",
       " 'disobeying': 30568,\n",
       " \"'royale\": 52010,\n",
       " \"harpo's\": 40831,\n",
       " 'canet': 52011,\n",
       " 'aileen': 19313,\n",
       " 'acurately': 52012,\n",
       " \"diplomat's\": 52013,\n",
       " 'rickman': 25242,\n",
       " 'arranged': 6746,\n",
       " 'rumbustious': 52014,\n",
       " 'familiarness': 52015,\n",
       " \"spider'\": 52016,\n",
       " 'hahahah': 68804,\n",
       " \"wood'\": 52017,\n",
       " 'transvestism': 40833,\n",
       " \"hangin'\": 34702,\n",
       " 'bringing': 2338,\n",
       " 'seamier': 40834,\n",
       " 'wooded': 34703,\n",
       " 'bravora': 52018,\n",
       " 'grueling': 16817,\n",
       " 'wooden': 1636,\n",
       " 'wednesday': 16818,\n",
       " \"'prix\": 52019,\n",
       " 'altagracia': 34704,\n",
       " 'circuitry': 52020,\n",
       " 'crotch': 11585,\n",
       " 'busybody': 57766,\n",
       " \"tart'n'tangy\": 52021,\n",
       " 'burgade': 14129,\n",
       " 'thrace': 52023,\n",
       " \"tom's\": 11038,\n",
       " 'snuggles': 52025,\n",
       " 'francesco': 29114,\n",
       " 'complainers': 52027,\n",
       " 'templarios': 52125,\n",
       " '272': 40835,\n",
       " '273': 52028,\n",
       " 'zaniacs': 52130,\n",
       " '275': 34706,\n",
       " 'consenting': 27631,\n",
       " 'snuggled': 40836,\n",
       " 'inanimate': 15492,\n",
       " 'uality': 52030,\n",
       " 'bronte': 11926,\n",
       " 'errors': 4010,\n",
       " 'dialogs': 3230,\n",
       " \"yomada's\": 52031,\n",
       " \"madman's\": 34707,\n",
       " 'dialoge': 30585,\n",
       " 'usenet': 52033,\n",
       " 'videodrome': 40837,\n",
       " \"kid'\": 26338,\n",
       " 'pawed': 52034,\n",
       " \"'girlfriend'\": 30569,\n",
       " \"'pleasure\": 52035,\n",
       " \"'reloaded'\": 52036,\n",
       " \"kazakos'\": 40839,\n",
       " 'rocque': 52037,\n",
       " 'mailings': 52038,\n",
       " 'brainwashed': 11927,\n",
       " 'mcanally': 16819,\n",
       " \"tom''\": 52039,\n",
       " 'kurupt': 25243,\n",
       " 'affiliated': 21905,\n",
       " 'babaganoosh': 52040,\n",
       " \"noe's\": 40840,\n",
       " 'quart': 40841,\n",
       " 'kids': 359,\n",
       " 'uplifting': 5034,\n",
       " 'controversy': 7093,\n",
       " 'kida': 21906,\n",
       " 'kidd': 23379,\n",
       " \"error'\": 52041,\n",
       " 'neurologist': 52042,\n",
       " 'spotty': 18510,\n",
       " 'cobblers': 30570,\n",
       " 'projection': 9878,\n",
       " 'fastforwarding': 40842,\n",
       " 'sters': 52043,\n",
       " \"eggar's\": 52044,\n",
       " 'etherything': 52045,\n",
       " 'gateshead': 40843,\n",
       " 'airball': 34708,\n",
       " 'unsinkable': 25244,\n",
       " 'stern': 7180,\n",
       " \"cervi's\": 52046,\n",
       " 'dnd': 40844,\n",
       " 'dna': 11586,\n",
       " 'insecurity': 20598,\n",
       " \"'reboot'\": 52047,\n",
       " 'trelkovsky': 11037,\n",
       " 'jaekel': 52048,\n",
       " 'sidebars': 52049,\n",
       " \"sforza's\": 52050,\n",
       " 'distortions': 17633,\n",
       " 'mutinies': 52051,\n",
       " 'sermons': 30602,\n",
       " '7ft': 40846,\n",
       " 'boobage': 52052,\n",
       " \"o'bannon's\": 52053,\n",
       " 'populations': 23380,\n",
       " 'chulak': 52054,\n",
       " 'mesmerize': 27633,\n",
       " 'quinnell': 52055,\n",
       " 'yahoo': 10307,\n",
       " 'meteorologist': 52057,\n",
       " 'beswick': 42577,\n",
       " 'boorman': 15493,\n",
       " 'voicework': 40847,\n",
       " \"ster'\": 52058,\n",
       " 'blustering': 22922,\n",
       " 'hj': 52059,\n",
       " 'intake': 27634,\n",
       " 'morally': 5621,\n",
       " 'jumbling': 40849,\n",
       " 'bowersock': 52060,\n",
       " \"'porky's'\": 52061,\n",
       " 'gershon': 16821,\n",
       " 'ludicrosity': 40850,\n",
       " 'coprophilia': 52062,\n",
       " 'expressively': 40851,\n",
       " \"india's\": 19500,\n",
       " \"post's\": 34710,\n",
       " 'wana': 52063,\n",
       " 'wang': 5283,\n",
       " 'wand': 30571,\n",
       " 'wane': 25245,\n",
       " 'edgeways': 52321,\n",
       " 'titanium': 34711,\n",
       " 'pinta': 40852,\n",
       " 'want': 178,\n",
       " 'pinto': 30572,\n",
       " 'whoopdedoodles': 52065,\n",
       " 'tchaikovsky': 21908,\n",
       " 'travel': 2103,\n",
       " \"'victory'\": 52066,\n",
       " 'copious': 11928,\n",
       " 'gouge': 22433,\n",
       " \"chapters'\": 52067,\n",
       " 'barbra': 6702,\n",
       " 'uselessness': 30573,\n",
       " \"wan'\": 52068,\n",
       " 'assimilated': 27635,\n",
       " 'petiot': 16116,\n",
       " 'most\\x85and': 52069,\n",
       " 'dinosaurs': 3930,\n",
       " 'wrong': 352,\n",
       " 'seda': 52070,\n",
       " 'stollen': 52071,\n",
       " 'sentencing': 34712,\n",
       " 'ouroboros': 40853,\n",
       " 'assimilates': 40854,\n",
       " 'colorfully': 40855,\n",
       " 'glenne': 27636,\n",
       " 'dongen': 52072,\n",
       " 'subplots': 4760,\n",
       " 'kiloton': 52073,\n",
       " 'chandon': 23381,\n",
       " \"effect'\": 34713,\n",
       " 'snugly': 27637,\n",
       " 'kuei': 40856,\n",
       " 'welcomed': 9092,\n",
       " 'dishonor': 30071,\n",
       " 'concurrence': 52075,\n",
       " 'stoicism': 23382,\n",
       " \"guys'\": 14896,\n",
       " \"beroemd'\": 52077,\n",
       " 'butcher': 6703,\n",
       " \"melfi's\": 40857,\n",
       " 'aargh': 30623,\n",
       " 'playhouse': 20599,\n",
       " 'wickedly': 11308,\n",
       " 'fit': 1180,\n",
       " 'labratory': 52078,\n",
       " 'lifeline': 40859,\n",
       " 'screaming': 1927,\n",
       " 'fix': 4287,\n",
       " 'cineliterate': 52079,\n",
       " 'fic': 52080,\n",
       " 'fia': 52081,\n",
       " 'fig': 34714,\n",
       " 'fmvs': 52082,\n",
       " 'fie': 52083,\n",
       " 'reentered': 52084,\n",
       " 'fin': 30574,\n",
       " 'doctresses': 52085,\n",
       " 'fil': 52086,\n",
       " 'zucker': 12606,\n",
       " 'ached': 31931,\n",
       " 'counsil': 52088,\n",
       " 'paterfamilias': 52089,\n",
       " 'songwriter': 13885,\n",
       " 'shivam': 34715,\n",
       " 'hurting': 9654,\n",
       " 'effects': 299,\n",
       " 'slauther': 52090,\n",
       " \"'flame'\": 52091,\n",
       " 'sommerset': 52092,\n",
       " 'interwhined': 52093,\n",
       " 'whacking': 27638,\n",
       " 'bartok': 52094,\n",
       " 'barton': 8775,\n",
       " 'frewer': 21909,\n",
       " \"fi'\": 52095,\n",
       " 'ingrid': 6192,\n",
       " 'stribor': 30575,\n",
       " 'approporiately': 52096,\n",
       " 'wobblyhand': 52097,\n",
       " 'tantalisingly': 52098,\n",
       " 'ankylosaurus': 52099,\n",
       " 'parasites': 17634,\n",
       " 'childen': 52100,\n",
       " \"jenkins'\": 52101,\n",
       " 'metafiction': 52102,\n",
       " 'golem': 17635,\n",
       " 'indiscretion': 40860,\n",
       " \"reeves'\": 23383,\n",
       " \"inamorata's\": 57781,\n",
       " 'brittannica': 52104,\n",
       " 'adapt': 7916,\n",
       " \"russo's\": 30576,\n",
       " 'guitarists': 48246,\n",
       " 'abbott': 10553,\n",
       " 'abbots': 40861,\n",
       " 'lanisha': 17649,\n",
       " 'magickal': 40863,\n",
       " 'mattter': 52105,\n",
       " \"'willy\": 52106,\n",
       " 'pumpkins': 34716,\n",
       " 'stuntpeople': 52107,\n",
       " 'estimate': 30577,\n",
       " 'ugghhh': 40864,\n",
       " 'gameplay': 11309,\n",
       " \"wern't\": 52108,\n",
       " \"n'sync\": 40865,\n",
       " 'sickeningly': 16117,\n",
       " 'chiara': 40866,\n",
       " 'disturbed': 4011,\n",
       " 'portmanteau': 40867,\n",
       " 'ineffectively': 52109,\n",
       " \"duchonvey's\": 82143,\n",
       " \"nasty'\": 37519,\n",
       " 'purpose': 1285,\n",
       " 'lazers': 52112,\n",
       " 'lightened': 28105,\n",
       " 'kaliganj': 52113,\n",
       " 'popularism': 52114,\n",
       " \"damme's\": 18511,\n",
       " 'stylistics': 30578,\n",
       " 'mindgaming': 52115,\n",
       " 'spoilerish': 46449,\n",
       " \"'corny'\": 52117,\n",
       " 'boerner': 34718,\n",
       " 'olds': 6792,\n",
       " 'bakelite': 52118,\n",
       " 'renovated': 27639,\n",
       " 'forrester': 27640,\n",
       " \"lumiere's\": 52119,\n",
       " 'gaskets': 52024,\n",
       " 'needed': 884,\n",
       " 'smight': 34719,\n",
       " 'master': 1297,\n",
       " \"edie's\": 25905,\n",
       " 'seeber': 40868,\n",
       " 'hiya': 52120,\n",
       " 'fuzziness': 52121,\n",
       " 'genesis': 14897,\n",
       " 'rewards': 12607,\n",
       " 'enthrall': 30579,\n",
       " \"'about\": 40869,\n",
       " \"recollection's\": 52122,\n",
       " 'mutilated': 11039,\n",
       " 'fatherlands': 52123,\n",
       " \"fischer's\": 52124,\n",
       " 'positively': 5399,\n",
       " '270': 34705,\n",
       " 'ahmed': 34720,\n",
       " 'zatoichi': 9836,\n",
       " 'bannister': 13886,\n",
       " 'anniversaries': 52127,\n",
       " \"helm's\": 30580,\n",
       " \"'work'\": 52128,\n",
       " 'exclaimed': 34721,\n",
       " \"'unfunny'\": 52129,\n",
       " '274': 52029,\n",
       " 'feeling': 544,\n",
       " \"wanda's\": 52131,\n",
       " 'dolan': 33266,\n",
       " '278': 52133,\n",
       " 'peacoat': 52134,\n",
       " 'brawny': 40870,\n",
       " 'mishra': 40871,\n",
       " 'worlders': 40872,\n",
       " 'protags': 52135,\n",
       " 'skullcap': 52136,\n",
       " 'dastagir': 57596,\n",
       " 'affairs': 5622,\n",
       " 'wholesome': 7799,\n",
       " 'hymen': 52137,\n",
       " 'paramedics': 25246,\n",
       " 'unpersons': 52138,\n",
       " 'heavyarms': 52139,\n",
       " 'affaire': 52140,\n",
       " 'coulisses': 52141,\n",
       " 'hymer': 40873,\n",
       " 'kremlin': 52142,\n",
       " 'shipments': 30581,\n",
       " 'pixilated': 52143,\n",
       " \"'00s\": 30582,\n",
       " 'diminishing': 18512,\n",
       " 'cinematic': 1357,\n",
       " 'resonates': 14898,\n",
       " 'simplify': 40874,\n",
       " \"nature'\": 40875,\n",
       " 'temptresses': 40876,\n",
       " 'reverence': 16822,\n",
       " 'resonated': 19502,\n",
       " 'dailey': 34722,\n",
       " '2\\x85': 52144,\n",
       " 'treize': 27641,\n",
       " 'majo': 52145,\n",
       " 'kiya': 21910,\n",
       " 'woolnough': 52146,\n",
       " 'thanatos': 39797,\n",
       " 'sandoval': 35731,\n",
       " 'dorama': 40879,\n",
       " \"o'shaughnessy\": 52147,\n",
       " 'tech': 4988,\n",
       " 'fugitives': 32018,\n",
       " 'teck': 30583,\n",
       " \"'e'\": 76125,\n",
       " 'doesn’t': 40881,\n",
       " 'purged': 52149,\n",
       " 'saying': 657,\n",
       " \"martians'\": 41095,\n",
       " 'norliss': 23418,\n",
       " 'dickey': 27642,\n",
       " 'dicker': 52152,\n",
       " \"'sependipity\": 52153,\n",
       " 'padded': 8422,\n",
       " 'ordell': 57792,\n",
       " \"sturges'\": 40882,\n",
       " 'independentcritics': 52154,\n",
       " 'tempted': 5745,\n",
       " \"atkinson's\": 34724,\n",
       " 'hounded': 25247,\n",
       " 'apace': 52155,\n",
       " 'clicked': 15494,\n",
       " \"'humor'\": 30584,\n",
       " \"martino's\": 17177,\n",
       " \"'supporting\": 52156,\n",
       " 'warmongering': 52032,\n",
       " \"zemeckis's\": 34725,\n",
       " 'lube': 21911,\n",
       " 'shocky': 52157,\n",
       " 'plate': 7476,\n",
       " 'plata': 40883,\n",
       " 'sturgess': 40884,\n",
       " \"nerds'\": 40885,\n",
       " 'plato': 20600,\n",
       " 'plath': 34726,\n",
       " 'platt': 40886,\n",
       " 'mcnab': 52159,\n",
       " 'clumsiness': 27643,\n",
       " 'altogether': 3899,\n",
       " 'massacring': 42584,\n",
       " 'bicenntinial': 52160,\n",
       " 'skaal': 40887,\n",
       " 'droning': 14360,\n",
       " 'lds': 8776,\n",
       " 'jaguar': 21912,\n",
       " \"cale's\": 34727,\n",
       " 'nicely': 1777,\n",
       " 'mummy': 4588,\n",
       " \"lot's\": 18513,\n",
       " 'patch': 10086,\n",
       " 'kerkhof': 50202,\n",
       " \"leader's\": 52161,\n",
       " \"'movie\": 27644,\n",
       " 'uncomfirmed': 52162,\n",
       " 'heirloom': 40888,\n",
       " 'wrangle': 47360,\n",
       " 'emotion\\x85': 52163,\n",
       " \"'stargate'\": 52164,\n",
       " 'pinoy': 40889,\n",
       " 'conchatta': 40890,\n",
       " 'broeke': 41128,\n",
       " 'advisedly': 40891,\n",
       " \"barker's\": 17636,\n",
       " 'descours': 52166,\n",
       " 'lots': 772,\n",
       " 'lotr': 9259,\n",
       " 'irs': 9879,\n",
       " 'lott': 52167,\n",
       " 'xvi': 40892,\n",
       " 'irk': 34728,\n",
       " 'irl': 52168,\n",
       " 'ira': 6887,\n",
       " 'belzer': 21913,\n",
       " 'irc': 52169,\n",
       " 'ire': 27645,\n",
       " 'requisites': 40893,\n",
       " 'discipline': 7693,\n",
       " 'lyoko': 52961,\n",
       " 'extend': 11310,\n",
       " 'nature': 873,\n",
       " \"'dickie'\": 52170,\n",
       " 'optimist': 40894,\n",
       " 'lapping': 30586,\n",
       " 'superficial': 3900,\n",
       " 'vestment': 52171,\n",
       " 'extent': 2823,\n",
       " 'tendons': 52172,\n",
       " \"heller's\": 52173,\n",
       " 'quagmires': 52174,\n",
       " 'miyako': 52175,\n",
       " 'moocow': 20601,\n",
       " \"coles'\": 52176,\n",
       " 'lookit': 40895,\n",
       " 'ravenously': 52177,\n",
       " 'levitating': 40896,\n",
       " 'perfunctorily': 52178,\n",
       " 'lookin': 30587,\n",
       " \"lot'\": 40898,\n",
       " 'lookie': 52179,\n",
       " 'fearlessly': 34870,\n",
       " 'libyan': 52181,\n",
       " 'fondles': 40899,\n",
       " 'gopher': 35714,\n",
       " 'wearying': 40901,\n",
       " \"nz's\": 52182,\n",
       " 'minuses': 27646,\n",
       " 'puposelessly': 52183,\n",
       " 'shandling': 52184,\n",
       " 'decapitates': 31268,\n",
       " 'humming': 11929,\n",
       " \"'nother\": 40902,\n",
       " 'smackdown': 21914,\n",
       " 'underdone': 30588,\n",
       " 'frf': 40903,\n",
       " 'triviality': 52185,\n",
       " 'fro': 25248,\n",
       " 'bothers': 8777,\n",
       " \"'kensington\": 52186,\n",
       " 'much': 73,\n",
       " 'muco': 34730,\n",
       " 'wiseguy': 22615,\n",
       " \"richie's\": 27648,\n",
       " 'tonino': 40904,\n",
       " 'unleavened': 52187,\n",
       " 'fry': 11587,\n",
       " \"'tv'\": 40905,\n",
       " 'toning': 40906,\n",
       " 'obese': 14361,\n",
       " 'sensationalized': 30589,\n",
       " 'spiv': 40907,\n",
       " 'spit': 6259,\n",
       " 'arkin': 7364,\n",
       " 'charleton': 21915,\n",
       " 'jeon': 16823,\n",
       " 'boardroom': 21916,\n",
       " 'doubts': 4989,\n",
       " 'spin': 3084,\n",
       " 'hepo': 53083,\n",
       " 'wildcat': 27649,\n",
       " 'venoms': 10584,\n",
       " 'misconstrues': 52191,\n",
       " 'mesmerising': 18514,\n",
       " 'misconstrued': 40908,\n",
       " 'rescinds': 52192,\n",
       " 'prostrate': 52193,\n",
       " 'majid': 40909,\n",
       " 'climbed': 16479,\n",
       " 'canoeing': 34731,\n",
       " 'majin': 52195,\n",
       " 'animie': 57804,\n",
       " 'sylke': 40910,\n",
       " 'conditioned': 14899,\n",
       " 'waddell': 40911,\n",
       " '3\\x85': 52196,\n",
       " 'hyperdrive': 41188,\n",
       " 'conditioner': 34732,\n",
       " 'bricklayer': 53153,\n",
       " 'hong': 2576,\n",
       " 'memoriam': 52198,\n",
       " 'inventively': 30592,\n",
       " \"levant's\": 25249,\n",
       " 'portobello': 20638,\n",
       " 'remand': 52200,\n",
       " 'mummified': 19504,\n",
       " 'honk': 27650,\n",
       " 'spews': 19505,\n",
       " 'visitations': 40912,\n",
       " 'mummifies': 52201,\n",
       " 'cavanaugh': 25250,\n",
       " 'zeon': 23385,\n",
       " \"jungle's\": 40913,\n",
       " 'viertel': 34733,\n",
       " 'frenchmen': 27651,\n",
       " 'torpedoes': 52202,\n",
       " 'schlessinger': 52203,\n",
       " 'torpedoed': 34734,\n",
       " 'blister': 69876,\n",
       " 'cinefest': 52204,\n",
       " 'furlough': 34735,\n",
       " 'mainsequence': 52205,\n",
       " 'mentors': 40914,\n",
       " 'academic': 9094,\n",
       " 'stillness': 20602,\n",
       " 'academia': 40915,\n",
       " 'lonelier': 52206,\n",
       " 'nibby': 52207,\n",
       " \"losers'\": 52208,\n",
       " 'cineastes': 40916,\n",
       " 'corporate': 4449,\n",
       " 'massaging': 40917,\n",
       " 'bellow': 30593,\n",
       " 'absurdities': 19506,\n",
       " 'expetations': 53241,\n",
       " 'nyfiken': 40918,\n",
       " 'mehras': 75638,\n",
       " 'lasse': 52209,\n",
       " 'visability': 52210,\n",
       " 'militarily': 33946,\n",
       " \"elder'\": 52211,\n",
       " 'gainsbourg': 19023,\n",
       " 'hah': 20603,\n",
       " 'hai': 13420,\n",
       " 'haj': 34736,\n",
       " 'hak': 25251,\n",
       " 'hal': 4311,\n",
       " 'ham': 4892,\n",
       " 'duffer': 53259,\n",
       " 'haa': 52213,\n",
       " 'had': 66,\n",
       " 'advancement': 11930,\n",
       " 'hag': 16825,\n",
       " \"hand'\": 25252,\n",
       " 'hay': 13421,\n",
       " 'mcnamara': 20604,\n",
       " \"mozart's\": 52214,\n",
       " 'duffel': 30731,\n",
       " 'haq': 30594,\n",
       " 'har': 13887,\n",
       " 'has': 44,\n",
       " 'hat': 2401,\n",
       " 'hav': 40919,\n",
       " 'haw': 30595,\n",
       " 'figtings': 52215,\n",
       " 'elders': 15495,\n",
       " 'underpanted': 52216,\n",
       " 'pninson': 52217,\n",
       " 'unequivocally': 27652,\n",
       " \"barbara's\": 23673,\n",
       " \"bello'\": 52219,\n",
       " 'indicative': 12997,\n",
       " 'yawnfest': 40920,\n",
       " 'hexploitation': 52220,\n",
       " \"loder's\": 52221,\n",
       " 'sleuthing': 27653,\n",
       " \"justin's\": 32622,\n",
       " \"'ball\": 52222,\n",
       " \"'summer\": 52223,\n",
       " \"'demons'\": 34935,\n",
       " \"mormon's\": 52225,\n",
       " \"laughton's\": 34737,\n",
       " 'debell': 52226,\n",
       " 'shipyard': 39724,\n",
       " 'unabashedly': 30597,\n",
       " 'disks': 40401,\n",
       " 'crowd': 2290,\n",
       " 'crowe': 10087,\n",
       " \"vancouver's\": 56434,\n",
       " 'mosques': 34738,\n",
       " 'crown': 6627,\n",
       " 'culpas': 52227,\n",
       " 'crows': 27654,\n",
       " 'surrell': 53344,\n",
       " 'flowless': 52229,\n",
       " 'sheirk': 52230,\n",
       " \"'three\": 40923,\n",
       " \"peterson'\": 52231,\n",
       " 'ooverall': 52232,\n",
       " 'perchance': 40924,\n",
       " 'bottom': 1321,\n",
       " 'chabert': 53363,\n",
       " 'sneha': 52233,\n",
       " 'inhuman': 13888,\n",
       " 'ichii': 52234,\n",
       " 'ursla': 52235,\n",
       " 'completly': 30598,\n",
       " 'moviedom': 40925,\n",
       " 'raddick': 52236,\n",
       " 'brundage': 51995,\n",
       " 'brigades': 40926,\n",
       " 'starring': 1181,\n",
       " \"'goal'\": 52237,\n",
       " 'caskets': 52238,\n",
       " 'willcock': 52239,\n",
       " \"threesome's\": 52240,\n",
       " \"mosque'\": 52241,\n",
       " \"cover's\": 52242,\n",
       " 'spaceships': 17637,\n",
       " 'anomalous': 40927,\n",
       " 'ptsd': 27655,\n",
       " 'shirdan': 52243,\n",
       " 'obscenity': 21962,\n",
       " 'lemmings': 30599,\n",
       " 'duccio': 30600,\n",
       " \"levene's\": 52244,\n",
       " \"'gorby'\": 52245,\n",
       " \"teenager's\": 25255,\n",
       " 'marshall': 5340,\n",
       " 'honeymoon': 9095,\n",
       " 'shoots': 3231,\n",
       " 'despised': 12258,\n",
       " 'okabasho': 52246,\n",
       " 'fabric': 8289,\n",
       " 'cannavale': 18515,\n",
       " 'raped': 3537,\n",
       " \"tutt's\": 52247,\n",
       " 'grasping': 17638,\n",
       " 'despises': 18516,\n",
       " \"thief's\": 40928,\n",
       " 'rapes': 8926,\n",
       " 'raper': 52248,\n",
       " \"eyre'\": 27656,\n",
       " 'walchek': 52249,\n",
       " \"elmo's\": 23386,\n",
       " 'perfumes': 40929,\n",
       " 'spurting': 21918,\n",
       " \"exposition'\\x85\": 52250,\n",
       " 'denoting': 52251,\n",
       " 'thesaurus': 34740,\n",
       " \"shoot'\": 40930,\n",
       " 'bonejack': 49759,\n",
       " 'simpsonian': 52253,\n",
       " 'hebetude': 30601,\n",
       " \"hallow's\": 34741,\n",
       " 'desperation\\x85': 52254,\n",
       " 'incinerator': 34742,\n",
       " 'congratulations': 10308,\n",
       " 'humbled': 52255,\n",
       " \"else's\": 5924,\n",
       " 'trelkovski': 40845,\n",
       " \"rape'\": 52256,\n",
       " \"'chapters'\": 59386,\n",
       " '1600s': 52257,\n",
       " 'martian': 7253,\n",
       " 'nicest': 25256,\n",
       " 'eyred': 52259,\n",
       " 'passenger': 9457,\n",
       " 'disgrace': 6041,\n",
       " 'moderne': 52260,\n",
       " 'barrymore': 5120,\n",
       " 'yankovich': 52261,\n",
       " 'moderns': 40931,\n",
       " 'studliest': 52262,\n",
       " 'bedsheet': 52263,\n",
       " 'decapitation': 14900,\n",
       " 'slurring': 52264,\n",
       " \"'nunsploitation'\": 52265,\n",
       " \"'character'\": 34743,\n",
       " 'cambodia': 9880,\n",
       " 'rebelious': 52266,\n",
       " 'pasadena': 27657,\n",
       " 'crowne': 40932,\n",
       " \"'bedchamber\": 52267,\n",
       " 'conjectural': 52268,\n",
       " 'appologize': 52269,\n",
       " 'halfassing': 52270,\n",
       " 'paycheque': 57816,\n",
       " 'palms': 20606,\n",
       " \"'islands\": 52271,\n",
       " 'hawked': 40933,\n",
       " 'palme': 21919,\n",
       " 'conservatively': 40934,\n",
       " 'larp': 64007,\n",
       " 'palma': 5558,\n",
       " 'smelling': 21920,\n",
       " 'aragorn': 12998,\n",
       " 'hawker': 52272,\n",
       " 'hawkes': 52273,\n",
       " 'explosions': 3975,\n",
       " 'loren': 8059,\n",
       " \"pyle's\": 52274,\n",
       " 'shootout': 6704,\n",
       " \"mike's\": 18517,\n",
       " \"driscoll's\": 52275,\n",
       " 'cogsworth': 40935,\n",
       " \"britian's\": 52276,\n",
       " 'childs': 34744,\n",
       " \"portrait's\": 52277,\n",
       " 'chain': 3626,\n",
       " 'whoever': 2497,\n",
       " 'puttered': 52278,\n",
       " 'childe': 52279,\n",
       " 'maywether': 52280,\n",
       " 'chair': 3036,\n",
       " \"rance's\": 52281,\n",
       " 'machu': 34745,\n",
       " 'ballet': 4517,\n",
       " 'grapples': 34746,\n",
       " 'summerize': 76152,\n",
       " 'freelance': 30603,\n",
       " \"andrea's\": 52283,\n",
       " '\\x91very': 52284,\n",
       " 'coolidge': 45879,\n",
       " 'mache': 18518,\n",
       " 'balled': 52285,\n",
       " 'grappled': 40937,\n",
       " 'macha': 18519,\n",
       " 'underlining': 21921,\n",
       " 'macho': 5623,\n",
       " 'oversight': 19507,\n",
       " 'machi': 25257,\n",
       " 'verbally': 11311,\n",
       " 'tenacious': 21922,\n",
       " 'windshields': 40938,\n",
       " 'paychecks': 18557,\n",
       " 'jerk': 3396,\n",
       " \"good'\": 11931,\n",
       " 'prancer': 34748,\n",
       " 'prances': 21923,\n",
       " 'olympus': 52286,\n",
       " 'lark': 21924,\n",
       " 'embark': 10785,\n",
       " 'gloomy': 7365,\n",
       " 'jehaan': 52287,\n",
       " 'turaqui': 52288,\n",
       " \"child'\": 20607,\n",
       " 'locked': 2894,\n",
       " 'pranced': 52289,\n",
       " 'exact': 2588,\n",
       " 'unattuned': 52290,\n",
       " 'minute': 783,\n",
       " 'skewed': 16118,\n",
       " 'hodgins': 40940,\n",
       " 'skewer': 34749,\n",
       " 'think\\x85': 52291,\n",
       " 'rosenstein': 38765,\n",
       " 'helmit': 52292,\n",
       " 'wrestlemanias': 34750,\n",
       " 'hindered': 16826,\n",
       " \"martha's\": 30604,\n",
       " 'cheree': 52293,\n",
       " \"pluckin'\": 52294,\n",
       " 'ogles': 40941,\n",
       " 'heavyweight': 11932,\n",
       " 'aada': 82190,\n",
       " 'chopping': 11312,\n",
       " 'strongboy': 61534,\n",
       " 'hegemonic': 41342,\n",
       " 'adorns': 40942,\n",
       " 'xxth': 41346,\n",
       " 'nobuhiro': 34751,\n",
       " 'capitães': 52298,\n",
       " 'kavogianni': 52299,\n",
       " 'antwerp': 13422,\n",
       " 'celebrated': 6538,\n",
       " 'roarke': 52300,\n",
       " 'baggins': 40943,\n",
       " 'cheeseburgers': 31270,\n",
       " 'matras': 52301,\n",
       " \"nineties'\": 52302,\n",
       " \"'craig'\": 52303,\n",
       " 'celebrates': 12999,\n",
       " 'unintentionally': 3383,\n",
       " 'drafted': 14362,\n",
       " 'climby': 52304,\n",
       " '303': 52305,\n",
       " 'oldies': 18520,\n",
       " 'climbs': 9096,\n",
       " 'honour': 9655,\n",
       " 'plucking': 34752,\n",
       " '305': 30074,\n",
       " 'address': 5514,\n",
       " 'menjou': 40944,\n",
       " \"'freak'\": 42592,\n",
       " 'dwindling': 19508,\n",
       " 'benson': 9458,\n",
       " 'white’s': 52307,\n",
       " 'shamelessness': 40945,\n",
       " 'impacted': 21925,\n",
       " 'upatz': 52308,\n",
       " 'cusack': 3840,\n",
       " \"flavia's\": 37567,\n",
       " 'effette': 52309,\n",
       " 'influx': 34753,\n",
       " 'boooooooo': 52310,\n",
       " 'dimitrova': 52311,\n",
       " 'houseman': 13423,\n",
       " 'bigas': 25259,\n",
       " 'boylen': 52312,\n",
       " 'phillipenes': 52313,\n",
       " 'fakery': 40946,\n",
       " \"grandpa's\": 27658,\n",
       " 'darnell': 27659,\n",
       " 'undergone': 19509,\n",
       " 'handbags': 52315,\n",
       " 'perished': 21926,\n",
       " 'pooped': 37778,\n",
       " 'vigour': 27660,\n",
       " 'opposed': 3627,\n",
       " 'etude': 52316,\n",
       " \"caine's\": 11799,\n",
       " 'doozers': 52317,\n",
       " 'photojournals': 34754,\n",
       " 'perishes': 52318,\n",
       " 'constrains': 34755,\n",
       " 'migenes': 40948,\n",
       " 'consoled': 30605,\n",
       " 'alastair': 16827,\n",
       " 'wvs': 52319,\n",
       " 'ooooooh': 52320,\n",
       " 'approving': 34756,\n",
       " 'consoles': 40949,\n",
       " 'disparagement': 52064,\n",
       " 'futureistic': 52322,\n",
       " 'rebounding': 52323,\n",
       " \"'date\": 52324,\n",
       " 'gregoire': 52325,\n",
       " 'rutherford': 21927,\n",
       " 'americanised': 34757,\n",
       " 'novikov': 82196,\n",
       " 'following': 1042,\n",
       " 'munroe': 34758,\n",
       " \"morita'\": 52326,\n",
       " 'christenssen': 52327,\n",
       " 'oatmeal': 23106,\n",
       " 'fossey': 25260,\n",
       " 'livered': 40950,\n",
       " 'listens': 13000,\n",
       " \"'marci\": 76164,\n",
       " \"otis's\": 52330,\n",
       " 'thanking': 23387,\n",
       " 'maude': 16019,\n",
       " 'extensions': 34759,\n",
       " 'ameteurish': 52332,\n",
       " \"commender's\": 52333,\n",
       " 'agricultural': 27661,\n",
       " 'convincingly': 4518,\n",
       " 'fueled': 17639,\n",
       " 'mahattan': 54014,\n",
       " \"paris's\": 40952,\n",
       " 'vulkan': 52336,\n",
       " 'stapes': 52337,\n",
       " 'odysessy': 52338,\n",
       " 'harmon': 12259,\n",
       " 'surfing': 4252,\n",
       " 'halloran': 23494,\n",
       " 'unbelieveably': 49580,\n",
       " \"'offed'\": 52339,\n",
       " 'quadrant': 30607,\n",
       " 'inhabiting': 19510,\n",
       " 'nebbish': 34760,\n",
       " 'forebears': 40953,\n",
       " 'skirmish': 34761,\n",
       " 'ocassionally': 52340,\n",
       " \"'resist\": 52341,\n",
       " 'impactful': 21928,\n",
       " 'spicier': 52342,\n",
       " 'touristy': 40954,\n",
       " \"'football'\": 52343,\n",
       " 'webpage': 40955,\n",
       " 'exurbia': 52345,\n",
       " 'jucier': 52346,\n",
       " 'professors': 14901,\n",
       " 'structuring': 34762,\n",
       " 'jig': 30608,\n",
       " 'overlord': 40956,\n",
       " 'disconnect': 25261,\n",
       " 'sniffle': 82201,\n",
       " 'slimeball': 40957,\n",
       " 'jia': 40958,\n",
       " 'milked': 16828,\n",
       " 'banjoes': 40959,\n",
       " 'jim': 1237,\n",
       " 'workforces': 52348,\n",
       " 'jip': 52349,\n",
       " 'rotweiller': 52350,\n",
       " 'mundaneness': 34763,\n",
       " \"'ninja'\": 52351,\n",
       " \"dead'\": 11040,\n",
       " \"cipriani's\": 40960,\n",
       " 'modestly': 20608,\n",
       " \"professor'\": 52352,\n",
       " 'shacked': 40961,\n",
       " 'bashful': 34764,\n",
       " 'sorter': 23388,\n",
       " 'overpowering': 16120,\n",
       " 'workmanlike': 18521,\n",
       " 'henpecked': 27662,\n",
       " 'sorted': 18522,\n",
       " \"jōb's\": 52354,\n",
       " \"'always\": 52355,\n",
       " \"'baptists\": 34765,\n",
       " 'dreamcatchers': 52356,\n",
       " \"'silence'\": 52357,\n",
       " 'hickory': 21929,\n",
       " 'fun\\x97yet': 52358,\n",
       " 'breakumentary': 52359,\n",
       " 'didn': 15496,\n",
       " 'didi': 52360,\n",
       " 'pealing': 52361,\n",
       " 'dispite': 40962,\n",
       " \"italy's\": 25262,\n",
       " 'instability': 21930,\n",
       " 'quarter': 6539,\n",
       " 'quartet': 12608,\n",
       " 'padmé': 52362,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'pahalniuk': 52364,\n",
       " 'honduras': 52365,\n",
       " 'bursting': 10786,\n",
       " \"pablo's\": 41465,\n",
       " 'irremediably': 52367,\n",
       " 'presages': 40963,\n",
       " 'bowlegged': 57832,\n",
       " 'dalip': 65183,\n",
       " 'entering': 6260,\n",
       " 'newsradio': 76172,\n",
       " 'presaged': 54150,\n",
       " \"giallo's\": 27663,\n",
       " 'bouyant': 40964,\n",
       " 'amerterish': 52368,\n",
       " 'rajni': 18523,\n",
       " 'leeves': 30610,\n",
       " 'macauley': 34767,\n",
       " 'seriously': 612,\n",
       " 'sugercoma': 52369,\n",
       " 'grimstead': 52370,\n",
       " \"'fairy'\": 52371,\n",
       " 'zenda': 30611,\n",
       " \"'twins'\": 52372,\n",
       " 'realisation': 17640,\n",
       " 'highsmith': 27664,\n",
       " 'raunchy': 7817,\n",
       " 'incentives': 40965,\n",
       " 'flatson': 52374,\n",
       " 'snooker': 35097,\n",
       " 'crazies': 16829,\n",
       " 'crazier': 14902,\n",
       " 'grandma': 7094,\n",
       " 'napunsaktha': 52375,\n",
       " 'workmanship': 30612,\n",
       " 'reisner': 52376,\n",
       " \"sanford's\": 61306,\n",
       " '\\x91doña': 52377,\n",
       " 'modest': 6108,\n",
       " \"everything's\": 19153,\n",
       " 'hamer': 40966,\n",
       " \"couldn't'\": 52379,\n",
       " 'quibble': 13001,\n",
       " 'socking': 52380,\n",
       " 'tingler': 21931,\n",
       " 'gutman': 52381,\n",
       " 'lachlan': 40967,\n",
       " 'tableaus': 52382,\n",
       " 'headbanger': 52383,\n",
       " 'spoken': 2847,\n",
       " 'cerebrally': 34768,\n",
       " \"'road\": 23490,\n",
       " 'tableaux': 21932,\n",
       " \"proust's\": 40968,\n",
       " 'periodical': 40969,\n",
       " \"shoveller's\": 52385,\n",
       " 'tamara': 25263,\n",
       " 'affords': 17641,\n",
       " 'concert': 3249,\n",
       " \"yara's\": 87955,\n",
       " 'someome': 52386,\n",
       " 'lingering': 8424,\n",
       " \"abraham's\": 41511,\n",
       " 'beesley': 34769,\n",
       " 'cherbourg': 34770,\n",
       " 'kagan': 28624,\n",
       " 'snatch': 9097,\n",
       " \"miyazaki's\": 9260,\n",
       " 'absorbs': 25264,\n",
       " \"koltai's\": 40970,\n",
       " 'tingled': 64027,\n",
       " 'crossroads': 19511,\n",
       " 'rehab': 16121,\n",
       " 'falworth': 52389,\n",
       " 'sequals': 52390,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_index is a dictionary mapping word to an integer index\n",
    "word_index = imdb.get_word_index()\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's reverse the dictionary\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key,value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top 5 frequent words in the reviews. the key or index start at 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'and', 'a', 'of', 'to')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index[1],reverse_word_index[2], reverse_word_index[3],reverse_word_index[4],reverse_word_index[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review = ' '.join([reverse_word_index.get(i-3,'?') for i in train_data[0]])\n",
    "\n",
    "#train_data[0] is a list of indexes. Normally a 585 in the list correspond to a word we can get with\n",
    "#reverse_word_index[585] but in this case it'll be reverse_word_index[585 - 3] because 0, 1, and 2 are\n",
    "#reserved indices for “padding,” “start of\n",
    "#sequence,” and “unknown.” If a key is not in our dictionary, we'll return '?'. Then we join all the corresponding\n",
    "#words is the list by separating them by an espace to make it clear to read. However we must be aware that words\n",
    "#which are not frequent(top 10000) in our dataset are removed so few words might be missing and we'll have to \n",
    "#guess them. It should be intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can’t feed lists of integers into a neural network. We have to turn our lists into\n",
    "tensors. There are two ways to do that:\n",
    "* Pad your lists so that they all have the same length, turn them into an integer tensor of shape (samples, word_indices), and then use as the first layer in your network a layer capable of handling such integer tensors\n",
    "\n",
    "* One-hot encode your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Then we could use as the first layer in your network a Dense layer, capable of handling floating-point vector data.\n",
    "\n",
    "Let’s go with the latter solution to vectorize the data, which we’ll do manually for\n",
    "maximum clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_sequences(sequences, dimension = 10000):\n",
    "    #Creating an all-zero matrix of shape (len(sequences),dimension)\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i,sequence] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorized_sequences(train_data)\n",
    "x_test = vectorized_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also vectorize your labels, which is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., ..., 0., 1., 0.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our data is ready to be fed to a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Building our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 The model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Validating our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 3s 226us/step - loss: 0.5082 - acc: 0.7816 - val_loss: 0.3795 - val_acc: 0.8686\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.3002 - acc: 0.9047 - val_loss: 0.3001 - val_acc: 0.8900\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.2178 - acc: 0.9281 - val_loss: 0.3080 - val_acc: 0.8718\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.1750 - acc: 0.9436 - val_loss: 0.2840 - val_acc: 0.8836\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.1426 - acc: 0.9544 - val_loss: 0.2850 - val_acc: 0.8868\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.1149 - acc: 0.9649 - val_loss: 0.3129 - val_acc: 0.8782\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 2s 134us/step - loss: 0.0977 - acc: 0.9711 - val_loss: 0.3130 - val_acc: 0.8845\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 2s 132us/step - loss: 0.0806 - acc: 0.9765 - val_loss: 0.3850 - val_acc: 0.8662\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0659 - acc: 0.9821 - val_loss: 0.3638 - val_acc: 0.8784\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0555 - acc: 0.9853 - val_loss: 0.3846 - val_acc: 0.8790\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 2s 134us/step - loss: 0.0446 - acc: 0.9891 - val_loss: 0.4162 - val_acc: 0.8768\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 2s 132us/step - loss: 0.0382 - acc: 0.9915 - val_loss: 0.4515 - val_acc: 0.8699\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0297 - acc: 0.9930 - val_loss: 0.4701 - val_acc: 0.8733\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0245 - acc: 0.9949 - val_loss: 0.5026 - val_acc: 0.8716\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0172 - acc: 0.9982 - val_loss: 0.5547 - val_acc: 0.8664\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 2s 130us/step - loss: 0.0154 - acc: 0.9974 - val_loss: 0.5804 - val_acc: 0.8684\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 2s 132us/step - loss: 0.0094 - acc: 0.9992 - val_loss: 0.6626 - val_acc: 0.8586\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 2s 131us/step - loss: 0.0121 - acc: 0.9972 - val_loss: 0.6482 - val_acc: 0.8694\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 2s 132us/step - loss: 0.0051 - acc: 0.9997 - val_loss: 0.7291 - val_acc: 0.8569\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 2s 132us/step - loss: 0.0095 - acc: 0.9979 - val_loss: 0.7066 - val_acc: 0.8673\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train,partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Plotting the training and the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(history_dict['acc']) + 1)\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss') #bo is for blue dot\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss') #b is for solid blue line\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Plotting the training and the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf() #clear the figures\n",
    "acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that after 5 epochs our validation accuracy start to decrease considerably, while the training accuracy keeps increasing. This is an example of overfitting. One way to avoid this is to reduce the number of epochs to 4. Let's train a new network from scratch using only 4 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Retraining a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.4749 - acc: 0.8214\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 91us/step - loss: 0.2652 - acc: 0.9100\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 91us/step - loss: 0.1984 - acc: 0.9303\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 91us/step - loss: 0.1674 - acc: 0.9406\n",
      "25000/25000 [==============================] - 2s 76us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size = 512)\n",
    "\n",
    "results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32343911949157717, 0.87276]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fairly naive approach achieves an accuracy of 88%. With state-of-the-art\n",
    "approaches, we should be able to get close to 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Further Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following experiments will help ensure  that the architecture choices we’ve\n",
    "made are all fairly reasonable, although there’s still room for improvement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's used 3 hidden layers instead of 2 and see how\n",
    "doing so affects validation and test accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 3s 103us/step - loss: 0.4557 - acc: 0.8121\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 94us/step - loss: 0.2542 - acc: 0.9095\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 93us/step - loss: 0.1945 - acc: 0.9306\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 93us/step - loss: 0.1640 - acc: 0.9412\n",
      "25000/25000 [==============================] - 2s 78us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(16, activation = 'relu')) #3rd hidden layers\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size = 512)\n",
    "\n",
    "results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3009793103694916, 0.88284]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually using 3 hidden layers improved our model accuracy, and decrease the loss. This architecture is better than the one we used. At this point, i don't know how to choose the number of hidden layers and the nomber of hidden units in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using layers with more hidden units : 32 units**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 3s 114us/step - loss: 0.4417 - acc: 0.8132\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 3s 104us/step - loss: 0.2416 - acc: 0.9095\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 3s 101us/step - loss: 0.1886 - acc: 0.9307\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 3s 100us/step - loss: 0.1603 - acc: 0.9406\n",
      "25000/25000 [==============================] - 2s 83us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(32, activation = 'relu')) #3rd hidden layers\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size = 512)\n",
    "\n",
    "results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34794556042671204, 0.87268]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximatively the same results as our first architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the mse loss function instead of binary_crossentropy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 3s 103us/step - loss: 0.1698 - acc: 0.7994\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 92us/step - loss: 0.0817 - acc: 0.9074\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 92us/step - loss: 0.0584 - acc: 0.9300\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 91us/step - loss: 0.0479 - acc: 0.9427\n",
      "25000/25000 [==============================] - 2s 74us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(16, activation = 'relu')) #3rd hidden layers\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'mse',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size = 512)\n",
    "\n",
    "results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08591119261503219, 0.88532]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best accuracy so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the tanh activation (an activation that was popular in the early days of\n",
    "neural networks) instead of relu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 3s 109us/step - loss: 0.1318 - acc: 0.8219\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 2s 92us/step - loss: 0.0653 - acc: 0.9156\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 2s 94us/step - loss: 0.0508 - acc: 0.9336\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 2s 97us/step - loss: 0.0441 - acc: 0.9444\n",
      "25000/25000 [==============================] - 2s 70us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'tanh', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'tanh'))\n",
    "model.add(layers.Dense(16, activation = 'tanh')) #3rd hidden layers\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss = 'mse',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=4, batch_size = 512)\n",
    "\n",
    "results = model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09605370857954025, 0.87472]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5.Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s what we should take away from this project:\n",
    "* You usually need to do quite a bit of preprocessing on your raw data in order to be able to feed it—as tensors—into a neural network. Sequences of words can be encoded as binary vectors, but there are other encoding options, too.\n",
    "* Stacks of Dense layers with relu activations can solve a wide range of problems (including sentiment classification), and you’ll likely use them frequently.\n",
    "* In a binary classification problem (two output classes), your network should end with a Dense layer with one unit and a sigmoid activation: the output of your network should be a scalar between 0 and 1, encoding a probability.\n",
    "* With such a scalar sigmoid output on a binary classification problem, the loss function you should use is binary_crossentropy.\n",
    "* The rmsprop optimizer is generally a good enough choice, whatever your problem. That’s one less thing for you to worry about.\n",
    "* As they get better on their training data, neural networks eventually start overfitting and end up obtaining increasingly worse results on data they’ve never seen before. Be sure to always monitor performance on data that is outside of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
